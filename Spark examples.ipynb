{"cells":[{"cell_type":"code","source":["# 1) conver column python list\n\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.master(\"local[1]\") \\\n                    .appName('SparkByExamples.com') \\\n                    .getOrCreate()\n\ndata = [(\"James\",\"Smith\",\"USA\",\"CA\"),(\"Michael\",\"Rose\",\"USA\",\"NY\"), \\\n    (\"Robert\",\"Williams\",\"USA\",\"CA\"),(\"Maria\",\"Jones\",\"USA\",\"FL\") \\\n  ]\ncolumns=[\"firstname\",\"lastname\",\"country\",\"state\"]\ndf=spark.createDataFrame(data=data,schema=columns)\ndf.show()\nprint(df.collect())\n\nstates1=df.rdd.map(lambda x: x[3]).collect()\nprint(states1)\n#['CA', 'NY', 'CA', 'FL']\nfrom collections import OrderedDict \nres = list(OrderedDict.fromkeys(states1)) \nprint(res)\n#['CA', 'NY', 'FL']\n\n\n#Example 2\nstates2=df.rdd.map(lambda x: x.state).collect()\nprint(states2)\n#['CA', 'NY', 'CA', 'FL']\n\nstates3=df.select(df.state).collect()\nprint(states3)\n#[Row(state='CA'), Row(state='NY'), Row(state='CA'), Row(state='FL')]\n\nstates4=df.select(df.state).rdd.flatMap(lambda x: x).collect()\nprint(states4)\n#['CA', 'NY', 'CA', 'FL']\n\nstates5=df.select(df.state).toPandas()\nstates6=list(states5)\nprint(states6)\n#['CA', 'NY', 'CA', 'FL']\n\npandDF=df.select(df.state,df.firstname).toPandas()\nprint(list(pandDF['state']))\nprint(list(pandDF['firstname']))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"5ae8458f-3673-40d1-a9a8-431171310782","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+--------+-------+-----+\n|firstname|lastname|country|state|\n+---------+--------+-------+-----+\n|    James|   Smith|    USA|   CA|\n|  Michael|    Rose|    USA|   NY|\n|   Robert|Williams|    USA|   CA|\n|    Maria|   Jones|    USA|   FL|\n+---------+--------+-------+-----+\n\n[Row(firstname='James', lastname='Smith', country='USA', state='CA'), Row(firstname='Michael', lastname='Rose', country='USA', state='NY'), Row(firstname='Robert', lastname='Williams', country='USA', state='CA'), Row(firstname='Maria', lastname='Jones', country='USA', state='FL')]\n['CA', 'NY', 'CA', 'FL']\n['CA', 'NY', 'FL']\n['CA', 'NY', 'CA', 'FL']\n[Row(state='CA'), Row(state='NY'), Row(state='CA'), Row(state='FL')]\n['CA', 'NY', 'CA', 'FL']\n['state']\n['CA', 'NY', 'CA', 'FL']\n['James', 'Michael', 'Robert', 'Maria']\n"]}],"execution_count":0},{"cell_type":"code","source":["# 2) current date\n\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.functions import to_timestamp, current_date\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType\n\ndf = spark.range(1).select(current_date().alias(\"current_date\"))\n\n# Show the current date\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"57e9f425-3115-4275-8d45-dbbc5ea0a6f4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------------+\n|current_date|\n+------------+\n|  2023-06-10|\n+------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# 3) pandas-pyspark-dataframe.py\n\n\nimport pandas as pd    \ndata = [['Scott', 50], ['Jeff', 45], ['Thomas', 54],['Ann',34]] \n  \n# Create the pandas DataFrame \npandasDF = pd.DataFrame(data, columns = ['Name', 'Age']) \n  \n# print dataframe. \nprint(pandasDF)\n\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .master(\"local[1]\") \\\n    .appName(\"SparkByExamples.com\") \\\n    .getOrCreate()\n\nsparkDF=spark.createDataFrame(pandasDF) \nsparkDF.printSchema()\nsparkDF.show()\n\n#sparkDF=spark.createDataFrame(pandasDF.astype(str)) \nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\nmySchema = StructType([ StructField(\"First Name\", StringType(), True)\\\n                       ,StructField(\"Age\", IntegerType(), True)])\n\nsparkDF2 = spark.createDataFrame(pandasDF,schema=mySchema)\nsparkDF2.printSchema()\nsparkDF2.show()\n\n\nspark.conf.set(\"spark.sql.execution.arrow.enabled\",\"true\")\nspark.conf.set(\"spark.sql.execution.arrow.pyspark.fallback.enabled\",\"true\")\n\npandasDF2=sparkDF2.select(\"*\").toPandas\nprint(pandasDF2)\n\n\ntest=spark.conf.get(\"spark.sql.execution.arrow.enabled\")\nprint(test)\n\ntest123=spark.conf.get(\"spark.sql.execution.arrow.pyspark.fallback.enabled\")\nprint(test123)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"18d647e3-2f25-43c7-aa7e-c2c19952ef14","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/plain":[],"application/vnd.databricks.v1+bamboolib_hint":"{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}"}},{"output_type":"stream","output_type":"stream","name":"stdout","text":["     Name  Age\n0   Scott   50\n1    Jeff   45\n2  Thomas   54\n3     Ann   34\nroot\n |-- Name: string (nullable = true)\n |-- Age: long (nullable = true)\n\n+------+---+\n|  Name|Age|\n+------+---+\n| Scott| 50|\n|  Jeff| 45|\n|Thomas| 54|\n|   Ann| 34|\n+------+---+\n\nroot\n |-- First Name: string (nullable = true)\n |-- Age: integer (nullable = true)\n\n+----------+---+\n|First Name|Age|\n+----------+---+\n|     Scott| 50|\n|      Jeff| 45|\n|    Thomas| 54|\n|       Ann| 34|\n+----------+---+\n\n<bound method PandasConversionMixin.toPandas of DataFrame[First Name: string, Age: int]>\ntrue\ntrue\n"]}],"execution_count":0},{"cell_type":"code","source":["#4) add month\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\nfrom pyspark.sql.functions import col,expr\ndata=[(\"2019-01-23\",1),(\"2019-06-24\",2),(\"2019-09-20\",3)]\n\nspark.createDataFrame(data).toDF(\"date\",\"increment\") \\\n    .select(col(\"date\"),col(\"increment\"), \\\n      expr(\"add_months(to_date(date,'yyyy-MM-dd'),cast(increment as int))\").alias(\"inc_date\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"49bf37c5-8e12-4c8e-aeaa-2a16172b52fa","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------+---------+----------+\n|      date|increment|  inc_date|\n+----------+---------+----------+\n|2019-01-23|        1|2019-02-23|\n|2019-06-24|        2|2019-08-24|\n|2019-09-20|        3|2019-12-20|\n+----------+---------+----------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# 5) add new column\n\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n                    .appName('SparkByExamples.com') \\\n                    .getOrCreate()\n\ndata = [('James','Smith','M',3000),\n  ('Anna','Rose','F',4100),\n  ('Robert','Williams','M',6200), \n]\n\ncolumns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\ndf = spark.createDataFrame(data=data, schema = columns)\ndf.show()\n\n\nif 'salary1' not in df.columns:\n    print(\"aa\")\n    \n# Add new constanct column\nfrom pyspark.sql.functions import lit\ndf.withColumn(\"bonus_percent\", lit(0.3)) \\\n  .show()\n  \n#Add column from existing column\ndf.withColumn(\"bonus_amount\", df.salary*0.3) \\\n  .show()\n\n#Add column by concatinating existing columns\nfrom pyspark.sql.functions import concat_ws\ndf.withColumn(\"name\", concat_ws(\",\",\"firstname\",'lastname')) \\\n  .show()\n\n#Add current date\nfrom pyspark.sql.functions import current_date\ndf.withColumn(\"current_date\", current_date()) \\\n  .show()\n\n\nfrom pyspark.sql.functions import when\ndf.withColumn(\"grade\", \\\n   when((df.salary < 4000), lit(\"A\")) \\\n     .when((df.salary >= 4000) & (df.salary <= 5000), lit(\"B\")) \\\n     .otherwise(lit(\"C\")) \\\n  ).show()\n    \n# Add column using select\ndf.select(\"firstname\",\"salary\", lit(0.3).alias(\"bonus\")).show()\ndf.select(\"firstname\",\"salary\", lit(df.salary * 0.3).alias(\"bonus_amount\")).show()\ndf.select(\"firstname\",\"salary\", current_date().alias(\"today_date\")).show()\n\n#Add columns using SQL\ndf.createOrReplaceTempView(\"PER\")\nspark.sql(\"select firstname,salary, '0.3' as bonus from PER\").show()\nspark.sql(\"select firstname,salary, salary * 0.3 as bonus_amount from PER\").show()\nspark.sql(\"select firstname,salary, current_date() as today_date from PER\").show()\nspark.sql(\"select firstname,salary, case salary when salary < 4000 then 'A' else 'B' END as grade from PER\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"07b5b776-75bd-45d1-a7fe-2b66b81f8c80","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+--------+------+------+\n|firstname|lastname|gender|salary|\n+---------+--------+------+------+\n|    James|   Smith|     M|  3000|\n|     Anna|    Rose|     F|  4100|\n|   Robert|Williams|     M|  6200|\n+---------+--------+------+------+\n\naa\n+---------+--------+------+------+-------------+\n|firstname|lastname|gender|salary|bonus_percent|\n+---------+--------+------+------+-------------+\n|    James|   Smith|     M|  3000|          0.3|\n|     Anna|    Rose|     F|  4100|          0.3|\n|   Robert|Williams|     M|  6200|          0.3|\n+---------+--------+------+------+-------------+\n\n+---------+--------+------+------+------------+\n|firstname|lastname|gender|salary|bonus_amount|\n+---------+--------+------+------+------------+\n|    James|   Smith|     M|  3000|       900.0|\n|     Anna|    Rose|     F|  4100|      1230.0|\n|   Robert|Williams|     M|  6200|      1860.0|\n+---------+--------+------+------+------------+\n\n+---------+--------+------+------+---------------+\n|firstname|lastname|gender|salary|           name|\n+---------+--------+------+------+---------------+\n|    James|   Smith|     M|  3000|    James,Smith|\n|     Anna|    Rose|     F|  4100|      Anna,Rose|\n|   Robert|Williams|     M|  6200|Robert,Williams|\n+---------+--------+------+------+---------------+\n\n+---------+--------+------+------+------------+\n|firstname|lastname|gender|salary|current_date|\n+---------+--------+------+------+------------+\n|    James|   Smith|     M|  3000|  2023-06-12|\n|     Anna|    Rose|     F|  4100|  2023-06-12|\n|   Robert|Williams|     M|  6200|  2023-06-12|\n+---------+--------+------+------+------------+\n\n+---------+--------+------+------+-----+\n|firstname|lastname|gender|salary|grade|\n+---------+--------+------+------+-----+\n|    James|   Smith|     M|  3000|    A|\n|     Anna|    Rose|     F|  4100|    B|\n|   Robert|Williams|     M|  6200|    C|\n+---------+--------+------+------+-----+\n\n+---------+------+-----+\n|firstname|salary|bonus|\n+---------+------+-----+\n|    James|  3000|  0.3|\n|     Anna|  4100|  0.3|\n|   Robert|  6200|  0.3|\n+---------+------+-----+\n\n+---------+------+------------+\n|firstname|salary|bonus_amount|\n+---------+------+------------+\n|    James|  3000|       900.0|\n|     Anna|  4100|      1230.0|\n|   Robert|  6200|      1860.0|\n+---------+------+------------+\n\n+---------+------+----------+\n|firstname|salary|today_date|\n+---------+------+----------+\n|    James|  3000|2023-06-12|\n|     Anna|  4100|2023-06-12|\n|   Robert|  6200|2023-06-12|\n+---------+------+----------+\n\n+---------+------+-----+\n|firstname|salary|bonus|\n+---------+------+-----+\n|    James|  3000|  0.3|\n|     Anna|  4100|  0.3|\n|   Robert|  6200|  0.3|\n+---------+------+-----+\n\n+---------+------+------------+\n|firstname|salary|bonus_amount|\n+---------+------+------------+\n|    James|  3000|       900.0|\n|     Anna|  4100|      1230.0|\n|   Robert|  6200|      1860.0|\n+---------+------+------------+\n\n+---------+------+----------+\n|firstname|salary|today_date|\n+---------+------+----------+\n|    James|  3000|2023-06-12|\n|     Anna|  4100|2023-06-12|\n|   Robert|  6200|2023-06-12|\n+---------+------+----------+\n\n+---------+------+-----+\n|firstname|salary|grade|\n+---------+------+-----+\n|    James|  3000|    B|\n|     Anna|  4100|    B|\n|   Robert|  6200|    B|\n+---------+------+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import approx_count_distinct,collect_list\nfrom pyspark.sql.functions import collect_set,sum,avg,max,countDistinct,count\nfrom pyspark.sql.functions import first, last, kurtosis, min, mean, skewness \nfrom pyspark.sql.functions import stddev, stddev_samp, stddev_pop, sumDistinct\nfrom pyspark.sql.functions import variance,var_samp,  var_pop\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\nsimpleData = [(\"James\", \"Sales\", 3000),\n    (\"Michael\", \"Sales\", 4600),\n    (\"Robert\", \"Sales\", 4100),\n    (\"Maria\", \"Finance\", 3000),\n    (\"James\", \"Sales\", 3000),\n    (\"Scott\", \"Finance\", 3300),\n    (\"Jen\", \"Finance\", 3900),\n    (\"Jeff\", \"Marketing\", 3000),\n    (\"Kumar\", \"Marketing\", 2000),\n    (\"Saif\", \"Sales\", 4100)\n  ]\nschema = [\"employee_name\", \"department\", \"salary\"]\n  \n  \ndf = spark.createDataFrame(data=simpleData, schema = schema)\ndf.printSchema()\ndf.show(truncate=False)\n\nprint(\"approx_count_distinct: \" + \\\n      str(df.select(approx_count_distinct(\"salary\")).collect()[0][0]))\n\nprint(\"avg: \" + str(df.select(avg(\"salary\")).collect()[0][0]))\n\ndf.select(collect_list(\"salary\")).show(truncate=False)\n\ndf.select(collect_set(\"salary\")).show(truncate=False)\n\ndf2 = df.select(countDistinct(\"department\", \"salary\"))\ndf2.show(truncate=False)\nprint(\"Distinct Count of Department &amp; Salary: \"+str(df2.collect()[0][0]))\n\nprint(\"count: \"+str(df.select(count(\"salary\")).collect()[0]))\ndf.select(first(\"salary\")).show(truncate=False)\ndf.select(last(\"salary\")).show(truncate=False)\ndf.select(kurtosis(\"salary\")).show(truncate=False)\ndf.select(max(\"salary\")).show(truncate=False)\ndf.select(min(\"salary\")).show(truncate=False)\ndf.select(mean(\"salary\")).show(truncate=False)\ndf.select(skewness(\"salary\")).show(truncate=False)\ndf.select(stddev(\"salary\"), stddev_samp(\"salary\"), \\\n    stddev_pop(\"salary\")).show(truncate=False)\ndf.select(sum(\"salary\")).show(truncate=False)\ndf.select(sumDistinct(\"salary\")).show(truncate=False)\ndf.select(variance(\"salary\"),var_samp(\"salary\"),var_pop(\"salary\")) \\\n  .show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"08435abe-8297-4fb4-a0e5-d1666a5aeb4e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|James        |Sales     |3000  |\n|Michael      |Sales     |4600  |\n|Robert       |Sales     |4100  |\n|Maria        |Finance   |3000  |\n|James        |Sales     |3000  |\n|Scott        |Finance   |3300  |\n|Jen          |Finance   |3900  |\n|Jeff         |Marketing |3000  |\n|Kumar        |Marketing |2000  |\n|Saif         |Sales     |4100  |\n+-------------+----------+------+\n\napprox_count_distinct: 6\navg: 3400.0\n+------------------------------------------------------------+\n|collect_list(salary)                                        |\n+------------------------------------------------------------+\n|[3000, 4600, 4100, 3000, 3000, 3300, 3900, 3000, 2000, 4100]|\n+------------------------------------------------------------+\n\n+------------------------------------+\n|collect_set(salary)                 |\n+------------------------------------+\n|[4600, 3000, 3900, 4100, 3300, 2000]|\n+------------------------------------+\n\n+----------------------------------+\n|count(DISTINCT department, salary)|\n+----------------------------------+\n|8                                 |\n+----------------------------------+\n\nDistinct Count of Department &amp; Salary: 8\ncount: Row(count(salary)=10)\n+-------------+\n|first(salary)|\n+-------------+\n|3000         |\n+-------------+\n\n+------------+\n|last(salary)|\n+------------+\n|4100        |\n+------------+\n\n+-------------------+\n|kurtosis(salary)   |\n+-------------------+\n|-0.6467803030303032|\n+-------------------+\n\n+-----------+\n|max(salary)|\n+-----------+\n|4600       |\n+-----------+\n\n+-----------+\n|min(salary)|\n+-----------+\n|2000       |\n+-----------+\n\n+-----------+\n|avg(salary)|\n+-----------+\n|3400.0     |\n+-----------+\n\n+--------------------+\n|skewness(salary)    |\n+--------------------+\n|-0.12041791181069571|\n+--------------------+\n\n+-------------------+-------------------+------------------+\n|stddev_samp(salary)|stddev_samp(salary)|stddev_pop(salary)|\n+-------------------+-------------------+------------------+\n|765.9416862050705  |765.9416862050705  |726.636084983398  |\n+-------------------+-------------------+------------------+\n\n+-----------+\n|sum(salary)|\n+-----------+\n|34000      |\n+-----------+\n\n"]},{"output_type":"stream","output_type":"stream","name":"stderr","text":["/databricks/spark/python/pyspark/sql/functions.py:723: FutureWarning: Deprecated in 3.2, use sum_distinct instead.\n  warnings.warn(\"Deprecated in 3.2, use sum_distinct instead.\", FutureWarning)\n"]},{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------------------+\n|sum(DISTINCT salary)|\n+--------------------+\n|20900               |\n+--------------------+\n\n+-----------------+-----------------+---------------+\n|var_samp(salary) |var_samp(salary) |var_pop(salary)|\n+-----------------+-----------------+---------------+\n|586666.6666666666|586666.6666666666|528000.0       |\n+-----------------+-----------------+---------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# 7)\n\nimport pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.master(\"local[1]\") \\\n                    .appName('SparkByExamples.com') \\\n                    .getOrCreate()\n\ncolumns = [\"name\",\"languagesAtSchool\",\"currentState\"]\ndata = [(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \\\n    (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"), \\\n    (\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]\n\ndf = spark.createDataFrame(data=data,schema=columns)\ndf.printSchema()\ndf.show(truncate=False)\n\nfrom pyspark.sql.functions import col, concat_ws\ndf2 = df.withColumn(\"languagesAtSchool\",\n   concat_ws(\",\",col(\"languagesAtSchool\")))\ndf2.printSchema()\ndf2.show(truncate=False)\n\n\ndf.createOrReplaceTempView(\"ARRAY_STRING\")\nspark.sql(\"select name, concat_ws(',',languagesAtSchool) as languagesAtSchool,currentState from ARRAY_STRING\").show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"64bc5dfd-92c1-4cf5-af79-b113e9431a7e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# 8)\n\nstates = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\nbroadcastStates = spark.sparkContext.broadcast(states)\n\ndata = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n  ]\n\ncolumns = [\"firstname\",\"lastname\",\"country\",\"state\"]\ndf = spark.createDataFrame(data = data, schema = columns)\ndf.printSchema()\ndf.show(truncate=False)\n\ndef state_convert(code):\n    return broadcastStates.value[code]\n\nresult = df.rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).toDF(columns)\nresult.show(truncate=False)\n\n# Broadcast variable on filter\nkeys_list = list(broadcastStates.value.keys())\nfilteDf = df.where(df['state'].isin(keys_list))\nfilteDf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"3cbb5673-7856-4168-b23f-e3a0f43c9cf3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- firstname: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- country: string (nullable = true)\n |-- state: string (nullable = true)\n\n+---------+--------+-------+-----+\n|firstname|lastname|country|state|\n+---------+--------+-------+-----+\n|James    |Smith   |USA    |CA   |\n|Michael  |Rose    |USA    |NY   |\n|Robert   |Williams|USA    |CA   |\n|Maria    |Jones   |USA    |FL   |\n+---------+--------+-------+-----+\n\n+---------+--------+-------+----------+\n|firstname|lastname|country|state     |\n+---------+--------+-------+----------+\n|James    |Smith   |USA    |California|\n|Michael  |Rose    |USA    |New York  |\n|Robert   |Williams|USA    |California|\n|Maria    |Jones   |USA    |Florida   |\n+---------+--------+-------+----------+\n\n+---------+--------+-------+-----+\n|firstname|lastname|country|state|\n+---------+--------+-------+-----+\n|James    |Smith   |USA    |CA   |\n|Michael  |Rose    |USA    |NY   |\n|Robert   |Williams|USA    |CA   |\n|Maria    |Jones   |USA    |FL   |\n+---------+--------+-------+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# 9)\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StringType, ArrayType,StructType,StructField\nspark = SparkSession.builder \\\n                    .appName('SparkByExamples.com') \\\n                    .getOrCreate()\n\n\narrayCol = ArrayType(StringType(),False)\n\ndata = [\n (\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"],\"OH\",\"CA\"),\n (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"],\"NY\",\"NJ\"),\n (\"Robert,,Williams\",[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"],\"UT\",\"NV\")\n]\n\nschema = StructType([ \n    StructField(\"name\",StringType(),True), \n    StructField(\"languagesAtSchool\",ArrayType(StringType()),True), \n    StructField(\"languagesAtWork\",ArrayType(StringType()),True), \n    StructField(\"currentState\", StringType(), True), \n    StructField(\"previousState\", StringType(), True) \n  ])\n\ndf = spark.createDataFrame(data=data,schema=schema)\ndf.printSchema()\ndf.show()\n\nfrom pyspark.sql.functions import explode\ndf.select(df.name,explode(df.languagesAtSchool)).show()\n\nfrom pyspark.sql.functions import split\ndf.select(split(df.name,\",\").alias(\"nameAsArray\")).show()\n\nfrom pyspark.sql.functions import array\ndf.select(df.name,array(df.currentState,df.previousState).alias(\"States\")).show()\n\nfrom pyspark.sql.functions import array_contains\ndf.select(df.name,array_contains(df.languagesAtSchool,\"Java\")\n    .alias(\"array_contains\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f9518f15-c716-4db1-b51e-ac8489645219","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# 10) \n\nimport pyspark\nfrom pyspark.sql import SparkSession\n\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\nstates = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\nbroadcastStates = spark.sparkContext.broadcast(states)\n\ndata = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n  ]\n\ncolumns = [\"firstname\",\"lastname\",\"country\",\"state\"]\ndf = spark.createDataFrame(data = data, schema = columns)\ndf.printSchema()\ndf.show(truncate=False)\n\ndef state_convert(code):\n    return broadcastStates.value[code]\n\nresult = df.rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).toDF(columns)\nresult.show(truncate=False)\n\n# Broadcast variable on filter\n\nfilteDf= df.where((df['state'].isin(broadcastStates.value)))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"daa13638-01f5-4b97-a6ec-2f39299c4c53","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# 11) \n\nimport pyspark\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\nsimpleData = [(\"James\",34,\"2006-01-01\",\"true\",\"M\",3000.60),\n    (\"Michael\",33,\"1980-01-10\",\"true\",\"F\",3300.80),\n    (\"Robert\",37,\"06-01-1992\",\"false\",\"M\",5000.50)\n  ]\n\ncolumns = [\"firstname\",\"age\",\"jobStartDate\",\"isGraduated\",\"gender\",\"salary\"]\ndf = spark.createDataFrame(data = simpleData, schema = columns)\ndf.printSchema()\ndf.show(truncate=False)\n\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.types import StringType,BooleanType,DateType\ndf2 = df.withColumn(\"age\",col(\"age\").cast(StringType())) \\\n    .withColumn(\"isGraduated\",col(\"isGraduated\").cast(BooleanType())) \\\n    .withColumn(\"jobStartDate\",col(\"jobStartDate\").cast(DateType()))\ndf2.printSchema()\n\ndf3 = df2.selectExpr(\"cast(age as int) age\",\n    \"cast(isGraduated as string) isGraduated\",\n    \"cast(jobStartDate as string) jobStartDate\")\ndf3.printSchema()\ndf3.show(truncate=False)\n\ndf3.createOrReplaceTempView(\"CastExample\")\ndf4 = spark.sql(\"SELECT STRING(age),BOOLEAN(isGraduated),DATE(jobStartDate) from CastExample\")\ndf4.printSchema()\ndf4.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5d5e7849-5841-4edc-afe4-c0eefd8734ca","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# 12)\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import DoubleType, IntegerType\n# Create SparkSession\nspark = SparkSession.builder \\\n          .appName('SparkByExamples.com') \\\n          .getOrCreate()\n\nsimpleData = [(\"James\",\"34\",\"true\",\"M\",\"3000.6089\"),\n    (\"Michael\",\"33\",\"true\",\"F\",\"3300.8067\"),\n    (\"Robert\",\"37\",\"false\",\"M\",\"5000.5034\")\n  ]\n\ncolumns = [\"firstname\",\"age\",\"isGraduated\",\"gender\",\"salary\"]\ndf = spark.createDataFrame(data = simpleData, schema = columns)\ndf.printSchema()\ndf.show(truncate=False)\n\nfrom pyspark.sql.functions import col,round,expr\ndf.withColumn(\"salary\",df.salary.cast('double')).printSchema()    \ndf.withColumn(\"salary\",df.salary.cast(DoublerType())).printSchema()    \ndf.withColumn(\"salary\",col(\"salary\").cast('double')).printSchema()    \n\n#df.withColumn(\"salary\",round(df.salary.cast(DoubleType()),2)).show(truncate=False).printSchema()    \ndf.selectExpr(\"firstname\",\"isGraduated\",\"cast(salary as double) salary\").printSchema()    \n\ndf.createOrReplaceTempView(\"CastExample\")\nspark.sql(\"SELECT firstname,isGraduated,DOUBLE(salary) as salary from CastExample\").printSchema()\n\n\n#df.select(\"firstname\",expr(df.age),\"isGraduated\",col(\"salary\").cast('float').alias(\"salary\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"806bab78-a779-48f8-a855-d0b19c326ed4","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# 13)\n\nimport pyspark\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ndept = [(\"Finance\",10), \\\n    (\"Marketing\",20), \\\n    (\"Sales\",30), \\\n    (\"IT\",40) \\\n  ]\ndeptColumns = [\"dept_name\",\"dept_id\"]\ndeptDF = spark.createDataFrame(data=dept, schema = deptColumns)\ndeptDF.printSchema()\ndeptDF.show(truncate=False)\n\ndataCollect = deptDF.collect()\n\nprint(dataCollect)\n\ndataCollect2 = deptDF.select(\"dept_name\").collect()\nprint(dataCollect2)\n\nfor row in dataCollect:\n    print(row['dept_name'] + \",\" +str(row['dept_id']))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"47763b86-1a45-4d78-8252-038c024181ff","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ndata=[(\"James\",\"Bond\",\"100\",None),\n      (\"Ann\",\"Varsa\",\"200\",'F'),\n      (\"Tom Cruise\",\"XXX\",\"400\",''),\n      (\"Tom Brand\",None,\"400\",'M')] \ncolumns=[\"fname\",\"lname\",\"id\",\"gender\"]\ndf=spark.createDataFrame(data,columns)\n\n#alias\nfrom pyspark.sql.functions import expr\ndf.select(df.fname.alias(\"first_name\"), \\\n          df.lname.alias(\"last_name\"), \\\n          expr(\" fname ||','|| lname\").alias(\"fullName\") \\\n   ).show()\n\n#asc, desc\ndf.sort(df.fname.asc()).show()\ndf.sort(df.fname.desc()).show()\n\n#cast\ndf.select(df.fname,df.id.cast(\"int\")).printSchema()\n\n#between\ndf.filter(df.id.between(100,300)).show()\n\n#contains\ndf.filter(df.fname.contains(\"Cruise\")).show()\n\n#startswith, endswith()\ndf.filter(df.fname.startswith(\"T\")).show()\ndf.filter(df.fname.endswith(\"Cruise\")).show()\n\n#eqNullSafe\n\n#isNull & isNotNull\ndf.filter(df.lname.isNull()).show()\ndf.filter(df.lname.isNotNull()).show()\n\n#like , rlike\ndf.select(df.fname,df.lname,df.id) \\\n  .filter(df.fname.like(\"%om\")) \n\n#over\n\n#substr\ndf.select(df.fname.substr(1,2).alias(\"substr\")).show()\n\n#when & otherwise\nfrom pyspark.sql.functions import when\ndf.select(df.fname,df.lname,when(df.gender==\"M\",\"Male\") \\\n              .when(df.gender==\"F\",\"Female\") \\\n              .when(df.gender==None ,\"\") \\\n              .otherwise(df.gender).alias(\"new_gender\") \\\n    ).show()\n\n#isin\nli=[\"100\",\"200\"]\ndf.select(df.fname,df.lname,df.id) \\\n  .filter(df.id.isin(li)) \\\n  .show()\n\nfrom pyspark.sql.types import StructType,StructField,StringType,ArrayType,MapType\ndata=[((\"James\",\"Bond\"),[\"Java\",\"C#\"],{'hair':'black','eye':'brown'}),\n      ((\"Ann\",\"Varsa\"),[\".NET\",\"Python\"],{'hair':'brown','eye':'black'}),\n      ((\"Tom Cruise\",\"\"),[\"Python\",\"Scala\"],{'hair':'red','eye':'grey'}),\n      ((\"Tom Brand\",None),[\"Perl\",\"Ruby\"],{'hair':'black','eye':'blue'})]\n\nschema = StructType([\n        StructField('name', StructType([\n            StructField('fname', StringType(), True),\n            StructField('lname', StringType(), True)])),\n        StructField('languages', ArrayType(StringType()),True),\n        StructField('properties', MapType(StringType(),StringType()),True)\n     ])\ndf=spark.createDataFrame(data,schema)\ndf.printSchema()\n#getItem()\ndf.select(df.languages.getItem(1)).show()\n\ndf.select(df.properties.getItem(\"hair\")).show()\n\n#getField from Struct or Map\ndf.select(df.properties.getField(\"hair\")).show()\n\ndf.select(df.name.getField(\"fname\")).show()\n\n#dropFields\n#from pyspark.sql.functions import col\n#df.withColumn(\"name1\",col(\"name\").dropFields([\"fname\"])).show()\n\n#withField\n#from pyspark.sql.functions import lit\n#df.withColumn(\"name\",df.name.withField(\"fname\",lit(\"AA\"))).show()\n\n#from pyspark.sql import Row\n#from pyspark.sql.functions import lit\n#df = spark.createDataFrame([Row(a=Row(b=1, c=2))])\n#df.withColumn('a', df['a'].withField('b', lit(3))).select('a.b').show()\n        \n#from pyspark.sql import Row\n#from pyspark.sql.functions import col, lit\n#df = spark.createDataFrame([\n#Row(a=Row(b=1, c=2, d=3, e=Row(f=4, g=5, h=6)))])\n#df.withColumn('a', df['a'].dropFields('b')).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ff156ffc-905e-497a-9485-6b7446bb24b3","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession,Row\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ndata=[(\"James\",23),(\"Ann\",40)]\ndf=spark.createDataFrame(data).toDF(\"name.fname\",\"gender\")\ndf.printSchema()\ndf.show()\n\nfrom pyspark.sql.functions import col\ndf.select(col(\"`name.fname`\")).show()\ndf.select(df[\"`name.fname`\"]).show()\ndf.withColumn(\"new_col\",col(\"`name.fname`\").substr(1,2)).show()\ndf.filter(col(\"`name.fname`\").startswith(\"J\")).show()\nnew_cols=(column.replace('.', '_') for column in df.columns)\ndf2 = df.toDF(*new_cols)\ndf2.show()\n\n\n# Using DataFrame object\ndf.select(df.gender).show()\ndf.select(df[\"gender\"]).show()\n#Accessing column name with dot (with backticks)\ndf.select(df[\"`name.fname`\"]).show()\n\n#Using SQL col() function\nfrom pyspark.sql.functions import col\ndf.select(col(\"gender\")).show()\n#Accessing column name with dot (with backticks)\ndf.select(col(\"`name.fname`\")).show()\n\n#Access struct column\ndata=[Row(name=\"James\",prop=Row(hair=\"black\",eye=\"blue\")),\n      Row(name=\"Ann\",prop=Row(hair=\"grey\",eye=\"black\"))]\ndf=spark.createDataFrame(data)\ndf.printSchema()\n\ndf.select(df.prop.hair).show()\ndf.select(df[\"prop.hair\"]).show()\ndf.select(col(\"prop.hair\")).show()\ndf.select(col(\"prop.*\")).show()\n\n# Column operators\ndata=[(100,2,1),(200,3,4),(300,4,4)]\ndf=spark.createDataFrame(data).toDF(\"col1\",\"col2\",\"col3\")\ndf.select(df.col1 + df.col2).show()\ndf.select(df.col1 - df.col2).show() \ndf.select(df.col1 * df.col2).show()\ndf.select(df.col1 / df.col2).show()\ndf.select(df.col1 % df.col2).show()\n\ndf.select(df.col2 > df.col3).show()\ndf.select(df.col2 < df.col3).show()\ndf.select(df.col2 == df.col3).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ebe0eb82-47ac-4ad1-84e9-5dd2afb6d7f9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# 17)\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ndataDictionary = [\n        ('James',{'hair':'black','eye':'brown'}),\n        ('Michael',{'hair':'brown','eye':None}),\n        ('Robert',{'hair':'red','eye':'black'}),\n        ('Washington',{'hair':'grey','eye':'grey'}),\n        ('Jefferson',{'hair':'brown','eye':''})\n        ]\n\ndf = spark.createDataFrame(data=dataDictionary, schema = ['name','properties'])\ndf.printSchema()\ndf.show(truncate=False)\n\ndf3=df.rdd.map(lambda x: \\\n    (x.name,x.properties[\"hair\"],x.properties[\"eye\"])) \\\n    .toDF([\"name\",\"hair\",\"eye\"])\ndf3.printSchema()\ndf3.show()\n\ndf.withColumn(\"hair\",df.properties.getItem(\"hair\")) \\\n  .withColumn(\"eye\",df.properties.getItem(\"eye\")) \\\n  .drop(\"properties\") \\\n  .show()\n\ndf.withColumn(\"hair\",df.properties[\"hair\"]) \\\n  .withColumn(\"eye\",df.properties[\"eye\"]) \\\n  .drop(\"properties\") \\\n  .show()\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"432d6de2-2a0f-49f0-b6d9-440771fa9852","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name: string (nullable = true)\n |-- properties: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n+----------+-----------------------------+\n|name      |properties                   |\n+----------+-----------------------------+\n|James     |{eye -> brown, hair -> black}|\n|Michael   |{eye -> null, hair -> brown} |\n|Robert    |{eye -> black, hair -> red}  |\n|Washington|{eye -> grey, hair -> grey}  |\n|Jefferson |{eye -> , hair -> brown}     |\n+----------+-----------------------------+\n\nroot\n |-- name: string (nullable = true)\n |-- hair: string (nullable = true)\n |-- eye: string (nullable = true)\n\n+----------+-----+-----+\n|      name| hair|  eye|\n+----------+-----+-----+\n|     James|black|brown|\n|   Michael|brown| null|\n|    Robert|  red|black|\n|Washington| grey| grey|\n| Jefferson|brown|     |\n+----------+-----+-----+\n\n+----------+-----+-----+\n|      name| hair|  eye|\n+----------+-----+-----+\n|     James|black|brown|\n|   Michael|brown| null|\n|    Robert|  red|black|\n|Washington| grey| grey|\n| Jefferson|brown|     |\n+----------+-----+-----+\n\n+----------+-----+-----+\n|      name| hair|  eye|\n+----------+-----+-----+\n|     James|black|brown|\n|   Michael|brown| null|\n|    Robert|  red|black|\n|Washington| grey| grey|\n| Jefferson|brown|     |\n+----------+-----+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\ndata = [ (\"36636\",\"Finance\",3000,\"USA\"), \n    (\"40288\",\"Finance\",5000,\"IND\"), \n    (\"42114\",\"Sales\",3900,\"USA\"), \n    (\"39192\",\"Marketing\",2500,\"CAN\"), \n    (\"34534\",\"Sales\",6500,\"USA\") ]\nschema = StructType([\n     StructField('id', StringType(), True),\n     StructField('dept', StringType(), True),\n     StructField('salary', IntegerType(), True),\n     StructField('location', StringType(), True)\n     ])\n\ndf = spark.createDataFrame(data=data,schema=schema)\ndf.printSchema()\ndf.show(truncate=False)\n\n#Convert scolumns to Map\nfrom pyspark.sql.functions import col,lit,create_map\ndf = df.withColumn(\"propertiesMap\",create_map(\n        lit(\"salary\"),col(\"salary\"),\n        lit(\"location\"),col(\"location\")\n        )).drop(\"salary\",\"location\")\ndf.printSchema()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1999b41f-c39a-4e35-a320-08042120a2a8","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# 18)\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder \\\n         .appName('SparkByExamples.com') \\\n         .getOrCreate()\n\ndata = [(\"James\", \"Sales\", 3000),\n    (\"Michael\", \"Sales\", 4600),\n    (\"Robert\", \"Sales\", 4100),\n    (\"Maria\", \"Finance\", 3000),\n    (\"James\", \"Sales\", 3000),\n    (\"Scott\", \"Finance\", 3300),\n    (\"Jen\", \"Finance\", 3900),\n    (\"Jeff\", \"Marketing\", 3000),\n    (\"Kumar\", \"Marketing\", 2000),\n    (\"Saif\", \"Sales\", 4100)\n  ]\ncolumns = [\"Name\",\"Dept\",\"Salary\"]\ndf = spark.createDataFrame(data=data,schema=columns)\ndf.distinct().show()\nprint(\"Distinct Count: \" + str(df.distinct().count()))\n\n# Using countDistrinct()\nfrom pyspark.sql.functions import countDistinct\ndf2=df.select(countDistinct(\"Dept\",\"Salary\"))\ndf2.show()\n\nprint(\"Distinct Count of Department &amp; Salary: \"+ str(df2.collect()[0][0]))\n\ndf.createOrReplaceTempView(\"PERSON\")\nspark.sql(\"select distinct(count(*)) from PERSON\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f056d879-3f5a-42d1-8aef-d1eab1238d1a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Spark examples","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
